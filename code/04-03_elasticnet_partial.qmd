---
title: "Elastic Net"
format: html
editor_options: 
  chunk_output_type: console
---
# April 3, 2025 

# Learning objectives  
Our learning objectives are to:  
  - Understand linear regression penalization types 
  - Use the ML framework to:  
    - pre-process data
    - train an elastic net model 
    - evaluate model predictability 

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables are highly correlated.  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand: I give the variables I think will have an impact   
    - by models: selection is given to us, like today's class.    
    
One approach to use models to perform variable selection is through applying a **regularization** to the model in the form of **penalties**.  

In **linear regression** (which is fit with ordinary least squares-OLS, prone to multicollinearity), model coefficients are estimated by MINIMIZING the **sum of squares of the error** (SSE), which is the sum of all square distances from each observation to the regression line (observed - predicted = residual).  

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/hyperplane-1.png)  
The is only one line that minimizes the sums of squares. The model takes the difference from all the points to the line, and minimizes the sum of the squares of those differences. That's the regression line. 


$$ minimize(SSE) $$

In **regularized linear regression**, penalties are applied on the SSE.

When penalties are applied, the coefficient of unimportant and/or correlated variables get constrained, reducing their influence in the model.  
There are three common penalty parameters we can implement:  

  - Ridge
  - Lasso (or LASSO)
  - Elastic net (or ENET), which is a combination of ridge and lasso.

## Penalties  

## Ridge penalty (L2)  
The size of this penalty, referred to as L2 (or Euclidean) norm, can take on a wide range of values, which is controlled by the **tuning parameter λ**.  

A penalty is added ot the SSE. This is a Loss function. Ridge does not set any coefficient to zero, but it shrinks the coefficients of correlated variables towards each other.

$$ minimize (SSE + L2) $$

where

$$ L2 = \lambda 
\begin{equation}
\sum_{j=1}^{p} \beta_{j}^2
\end{equation} $$  

When λ = 0, there is no effect and our objective function equals the normal ordinary least squares (OLS) regression objective function of simply minimizing SSE. 

However, as λ → ∞, the penalty becomes large and forces the coefficients toward zero (but not all the way), as in the figure below. 

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/ridge-coef-example-1.png)
Each line represents the slope of a predictor variable. The more the line is pushed towards zero, the less influence it has on the model.


However, ridge regression does not perform feature selection and will **retain all available features in the final model**.   

Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create (e.g., in smaller data sets with severe multicollinearity).   

If greater interpretation is necessary and many of the features are redundant or irrelevant then a lasso or elastic net penalty may be preferable.

## Lasso penalty (L1)  
The lasso (least absolute shrinkage and selection operator) penalty is an alternative to the ridge penalty that requires only a small modification. The only difference is that we swap out the  
L2 norm for an L1 norm.  

$$ minimize (SSE + L1) $$

where

$$ L1 = \lambda 
\begin{equation}
\sum_{j=1}^{p} |\beta_{j}|
\end{equation} $$

Whereas the ridge penalty pushes variables to approximately but not equal to zero, the **lasso** penalty will actually **push coefficients all the way to zero** as in the below figure:  

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/lasso-coef-example-1.png)

Switching to the lasso penalty not only improves the model but it also **conducts automated feature selection**.

##  Ridge + Lasso = Elastic net  

A generalization of the ridge and lasso penalties, called the elastic net, **combines the two penalties**.  

$$ minimize (SSE + L2 + L1) $$

Although lasso models perform feature selection, when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic.   

In contrast, the ridge regression penalty is a little more effective in systematically handling correlated features together.   

Consequently, the advantage of the elastic net penalty is that it **enables effective regularization** via the ridge penalty with the **feature selection** characteristics of the lasso penalty, as in the figure below:    

![](https://bradleyboehmke.github.io/HOML/06-regularized-regression_files/figure-html/elastic-net-coef-example-1.png)

# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("glmnet")
#install.packages("vip")
#install.packages("tidymodels")

library(tidymodels) # machine learning workflows
library(tidyverse)
library(glmnet) # 
library(vip)

```

```{r weather}
weather <- read_csv("data/weather_monthsum.csv")

weather
```

# ML workflow  
Let's use the workflow defined in the lecture below.  
In R, we will use many packages built specifically for different steps on this workflow, all of which use tidyverse principles.    

These packages are made available as a bundle through the meta-package `tidymodels` <https://www.tidymodels.org/packages/>, and include:  
  - `rsamples` for data split and resampling  
  - `recipes` for data processing  
  - `parsnip` to specify model types and engines  
  - `tune` to fine-tune hyper-parameters  
  - `dials` to create grids  
  - `yardstick` to assess performance  

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- 
  initial_split(weather, 
                prop = 0.7,  # 70 % training, 30% testing
                strata = strength_gtex) # stratified sampling based on our predicted variable. It is safe to use stratified when the predicted variable is continuous. We can also stratify by a predictor variable; or stratify by year or by site.

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- 
  training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- 
  testing(weather_split)

weather_test
```
How many observations?  

Let's check the distribution of our predicted variable **strength_gtex** across training and testing: 
```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") 
  
```
Let's go back to the split chunk and use **stratified sampling** based on our predicted variable **strength_gtex**.



Now, we put our **test set** aside and continue with our **train set** for training.  


  
### b. Data processing  
Before training, we need to perform some processing steps, like  
  - **removing unimportant variables**  
  - **performing PCA on the go**    
  - normalizing  
  - dropping NAs  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model (linear_reg).
We can search for that in this link: https://www.tmwr.org/pre-proc-table  
You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

```{r weather_recipe}
weather_recipe <- 
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ ., #the dot operator means all other variables
         data = weather_train) %>%
  # Removing year, site, and weather outside of growing season  
  step_rm(year, site,
          matches("Jan|Feb|Mar|Apr|Nov|Dec"))
  # Decorrelating
  #step_pca(all_numeric(), -all_outcomes(), num_comp = 7) # outcomes are strength_gtex

weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- 
  weather_recipe %>%
  prep( ) 

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use. Not: We can specify the package we want to use.  

An elastic net model is a linear regression model, penalized.  

Elastic net **hyperparameters**:  
  - **penalty**: equivalent to lambda  
  - **mixture**: 0 (ridge) to 1 (lasso). Specifies how much of ridge and lasso we would like.   

Let's create a model specification that will **fine-tune** these for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **glmnet** engine/package.  
  
```{r enet_spec}
enet_spec  <- 
  # Specifying linear regression as our model type, asking to tune the hyperparameters
  linear_reg(penalty = tune(), 
             mixture = tune()) %>%
  # Specify the engine
  set_engine("glmnet") # which package specifically we want to use

enet_spec
```
Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

### b. Hyper-parameter tuning  
Now, let's create a **grid** to perform our hyperparameter tuninig search with multiple combinations of values for penalty and mixture:

```{r enet_grid}
enet_grid <- crossing(penalty = seq(0, 
                                    10, 
                                    by = 1), # we can change starting, ending, or step size
                      mixture = seq(0, 
                                    1, 
                                    by = 0.1))

enet_grid
```

Our grid has **121** potential combinations of mixture and penalty.

```{r}
ggplot(enet_grid) +
  geom_point(aes(x = mixture, 
                 y = penalty)) +
  labs(title = "Elastic net grid search",
       x = "Mixture (alpha)",
       y = "Penalty (lambda)") +
  theme_bw()
```

For our grid search, we need:  
  - Our model specification (`enet_spec`)  
  - The recipe (`weather_recipe`)  
  - The grid (`enet_grid`), and    
  - Our **resampling strategy** (don't have yet)  
  
Let's define our resampling strategy below, using a 10-fold cross validation approach:  
```{r resampling_foldcv}
resampling_foldcv <- 
  vfold_cv(weather_train, 
           v = 10) # number of folds
   

resampling_foldcv
resampling_foldcv$splits[[1]] # it takes 90% of the data in each fold for training, and 10% for assessment (validation). 
```
On each fold, we'll use **437** observations for training and **49** observations to assess performance.    

Now, let's perform the grid search below:  
```{r enet_grid_result}
enet_grid_result <- 
  tune_grid(enet_spec, # model specification
             # the recipe
             preprocessor = weather_recipe,
             # the grid
             grid = enet_grid,
             # the resampling strategy
             resamples = resampling_foldcv) 

enet_grid_result

enet_grid_result$.metrics[[1]] # metrics for first one

```

Why 242 rows?
121 (each a combination of mixture and penalty) x 2 (2 assessment metrics)  

Let's collect a summary of metrics (across all folds, for each mixture x penalty combination), and plot them.  

Firs, RMSE (lower is better):
```{r RMSE}
enet_grid_result %>%
  collect_metrics() %>%  # the mean given here is across all folds
  filter(.metric == "rmse") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "RMSE")
```

What penalty and mixture values created lowest RMSE?  The best model would be a penalty of ~1 and a mixture of O. 

Now, let's look into R2 (higher is better):  

```{r R2}
enet_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "R2")
```
What penalty and mixture values created lowest RMSE?  

It seems that our best model is with hyperparameters set to:  
  - mixture = 0  
  - penalty = 0  
  
Let's extract the hyperparameters from the best model as judged by 2 performance metrics:  
```{r}
# Based on lowest RMSE
best_rmse <- 
  enet_grid_result %>%
  select_best(metric = "rmse") # the best model based on RMSE

best_rmse
```

```{r}
# Based on greatest R2
best_r2 <- 
  enet_grid_result %>%
  select_best(metric = "rsq") # the best model based on R2

best_r2

```
Based on RMSE, we would choose mixture = 0, penalty = 0.

Based on R2, we would choose mixture = 0, penalty = 0.7.

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec <- 
  linear_reg(penalty = 0.7, 
             mixture = 0) %>%
  set_engine("glmnet")

final_spec
```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Traninig the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
final_fit <- 
  last_fit(final_spec, # model specification
          preprocessor = weather_recipe, # recipe
          split = weather_split) # the split we created earlier

final_fit %>%
  collect_predictions() # .pred is the prediction of the data and strength_gtex is the observed data.
```
Why 212 observations?  

Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics() 
```

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE for the training data 

final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```
How does metrics on test compare to metrics on train?  

Why? when we train the model we can expect to have lower RMSE and higher R2.

Predicted vs. observed plot: (specially for the test set) 
```{r}
# This is only on the test set. Expect in in a machine learning paper. 

final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = strength_gtex,
             y = .pred)) +
  geom_point() +
  geom_abline() + # add 1 to 1 line
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 
```

Coefficients:  
```{r}
final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%  # apply the recipe to it 
  tidy() %>%  # gives us the slope as estimate
  arrange(desc(estimate)) # from high to low coefficients (greater to lower impact)

```

Variable importance:  
```{r}
# whether the variable is positive or negative importance 

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather)) %>%
  vi %>% # variable importance
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = Importance, 
             y = Variable, 
             fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```

**Therefore, tmin in September (positive effect), and tmax in September (negative effect) were the most important variables affecting cotton fiber strength.**  

# Summary  
In this exercise, we covered: 
  - Penalized linear regression types (ridge, lasso, elastic net)  
  - Set up a ML workflow to train an elastic net model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used a fixed grid to search the best values for mixture and penalty  
  - Used 5-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics to select best model  
  - Once final model was determined, used it to predict **test set**  
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, and variable importance  
  

